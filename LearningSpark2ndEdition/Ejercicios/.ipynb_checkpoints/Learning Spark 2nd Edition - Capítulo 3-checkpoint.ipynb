{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Capítulo 3\n",
    " \n",
    " a. Realizar todos los ejercicios propuestos del libro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6f6feadd\r\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Id,IntegerType,false), StructField(First,StringType,false), StructField(Last,StringType,false), StructField(Url,StringType,false), StructField(Published,StringType,false), StructField(Hits,IntegerType,false), StructField(Campaigns,ArrayType(StringType,true),false))\r\n",
       "jsonFile: String = /data/blogs.json\r\n",
       "blogsDF: org.apache.spark.sql.DataFrame = [Id: int, First: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// EXAMPLE 3_7\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"Capítulo3\")\n",
    "    .getOrCreate()\n",
    "val schema = StructType(Array(StructField(\"Id\",IntegerType,false),\n",
    "                              StructField(\"First\",StringType,false),\n",
    "                              StructField(\"Last\",StringType,false),\n",
    "                              StructField(\"Url\",StringType,false),\n",
    "                              StructField(\"Published\",StringType,false),\n",
    "                              StructField(\"Hits\",IntegerType,false),\n",
    "                              StructField(\"Campaigns\",ArrayType(StringType),false)))\n",
    "val jsonFile = \"/data/blogs.json\"\n",
    "val blogsDF = spark.read.schema(schema).json(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "|     15318|\n",
      "|     21136|\n",
      "|     81156|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n",
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogsDF.columns\n",
    "blogsDF.col(\"Id\")\n",
    "blogsDF.select(expr(\"Hits*2\")).show(5)\n",
    "blogsDF.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()\n",
    "blogsDF.withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\"))))\n",
    "    .select(col(\"AuthorsId\"))\n",
    "    .show(4)\n",
    "\n",
    "// same shit\n",
    "blogsDF.select(expr(\"Hits\")).show(2)\n",
    "blogsDF.select(col(\"Hits\")).show(2)\n",
    "blogsDF.select($\"Hits\").show(2)\n",
    "blogsDF.select(\"Hits\").show(2)\n",
    "blogsDF.select('Hits).show(2)\n",
    "\n",
    "blogsDF.sort($\"Id\".desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Fire Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: float (nullable = true)\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fireSFFile: String = ../data/sf-fire-calls.csv\r\n",
       "fireSFschema: org.apache.spark.sql.types.StructType = StructType(StructField(CallNumber,IntegerType,true), StructField(UnitID,StringType,true), StructField(IncidentNumber,IntegerType,true), StructField(CallType,StringType,true), StructField(CallDate,StringType,true), StructField(WatchDate,StringType,true), StructField(CallFinalDisposition,StringType,true), StructField(AvailableDtTm,StringType,true), StructField(Address,StringType,true), StructField(City,StringType,true), StructField(Zipcode,IntegerType,true), StructField(Battalion,StringType,true), StructField(StationArea,StringType,true), StructField(Box,StringType,true), StructField(OriginalPriority,StringType,true), StructField(Priority,StringType,true), StructField(FinalPriority,Intege...\r\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val fireSFFile = \"../data/sf-fire-calls.csv\"\n",
    "\n",
    "val fireSFschema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "                                    StructField(\"UnitID\", StringType, true),\n",
    "                                    StructField(\"IncidentNumber\", IntegerType, true),\n",
    "                                    StructField(\"CallType\", StringType, true),                  \n",
    "                                    StructField(\"CallDate\", StringType, true),      \n",
    "                                    StructField(\"WatchDate\", StringType, true),\n",
    "                                    StructField(\"CallFinalDisposition\", StringType, true),\n",
    "                                    StructField(\"AvailableDtTm\", StringType, true),\n",
    "                                    StructField(\"Address\", StringType, true),       \n",
    "                                    StructField(\"City\", StringType, true),       \n",
    "                                    StructField(\"Zipcode\", IntegerType, true),       \n",
    "                                    StructField(\"Battalion\", StringType, true),                 \n",
    "                                    StructField(\"StationArea\", StringType, true),       \n",
    "                                    StructField(\"Box\", StringType, true),       \n",
    "                                    StructField(\"OriginalPriority\", StringType, true),       \n",
    "                                    StructField(\"Priority\", StringType, true),       \n",
    "                                    StructField(\"FinalPriority\", IntegerType, true),       \n",
    "                                    StructField(\"ALSUnit\", BooleanType, true),       \n",
    "                                    StructField(\"CallTypeGroup\", StringType, true),\n",
    "                                    StructField(\"NumAlarms\", IntegerType, true),\n",
    "                                    StructField(\"UnitType\", StringType, true),\n",
    "                                    StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "                                    StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "                                    StructField(\"SupervisorDistrict\", StringType, true),\n",
    "                                    StructField(\"Neighborhood\", StringType, true),\n",
    "                                    StructField(\"Location\", StringType, true),\n",
    "                                    StructField(\"RowID\", StringType, true),\n",
    "                                    StructField(\"Delay\", FloatType, true)))\n",
    "\n",
    "val fireDF =  spark.read\n",
    "    .schema(fireSFschema)\n",
    "    .option(\"header\",\"true\")\n",
    "    .csv(fireSFFile)\n",
    "\n",
    "// Cache el DataFrame ya que vamos a hacer varias operaciones\n",
    "fireDF.cache()\n",
    "fireDF.count()\n",
    "fireDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fewFireDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [IncidentNumber: int, AvailableDtTm: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fewFireDF = fireDF\n",
    "    .select(\"IncidentNumber\",\"AvailableDtTm\",\"CallType\")\n",
    "    .where($\"CallType\" =!= \"Medical Incident\")\n",
    "fewFireDF.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-1) How many distinct types of calls were made to the Fire Department?**\n",
    "\n",
    "To be sure, let's not count \"null\" strings in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res64: Long = 30\r\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fireDF.select(\"CallType\")\n",
    "    .where($\"CallType\".isNotNull)\n",
    "    .distinct()\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-2) What are distinct types of calls were made to the Fire Department?**\n",
    "\n",
    "These are all the distinct type of call to the SF Fire Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireDF.select(\"CallType\")\n",
    "    .where($\"CallType\".isNotNull)\n",
    "    .distinct()\n",
    "    .show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-3) Find out all response or delayed times greater than 5 mins?**\n",
    "\n",
    "1. Rename the column Delay - > ReponseDelayedinMins\n",
    "2. Returns a new DataFrame\n",
    "3. Find out all calls where the response time to the fire site was delayed for more than 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newFireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\",\"ResponseDelayedinMins\")\n",
    "newFireDF.select(\"ResponseDelayedinMins\")\n",
    "    .where($\"ResponseDelayedinMins\" > 5)\n",
    "    .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fireTSDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cambio el Type de las columnas para poder operar fácilmente con fechas \"to_timestamp\"\n",
    "val fireTSDF = newFireDF\n",
    "    .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")) // esto añade una columna con un nombre nuevo\n",
    "    .drop(\"CallDate\") // quitamos del DataFrame la columna de la que hemos creado la nueva columna\n",
    "    .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"WatchDate\") \n",
    "    .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "    .drop(\"AvailableDtTm\")\n",
    "\n",
    "fireTSDF.cache()\n",
    "fireTSDF.select(\"IncidentDate\",\"OnWatchDate\",\"AvailableDtTS\").show(5,false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-4) What were the most common call types?**\n",
    "\n",
    "List them in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTSDF.select(\"CallType\")\n",
    "    .where($\"CallType\".isNotNull)\n",
    "    .groupBy(\"CallType\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-4a) What zip codes accounted for most common calls?**\n",
    "\n",
    "Let's investigate what zip codes in San Francisco accounted for most fire calls and what type where they.\n",
    "\n",
    "1. Filter out by CallType\n",
    "2. Group them by CallType and Zip code\n",
    "3. Count them and display them in descending order\n",
    "\n",
    "It seems like the most common calls were all related to Medical Incident, and the two zip codes are 94102 and 94103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-----+\n",
      "|CallType        |ZipCode|count|\n",
      "+----------------+-------+-----+\n",
      "|Medical Incident|94102  |16130|\n",
      "|Medical Incident|94103  |14775|\n",
      "|Medical Incident|94110  |9995 |\n",
      "|Medical Incident|94109  |9479 |\n",
      "|Medical Incident|94124  |5885 |\n",
      "|Medical Incident|94112  |5630 |\n",
      "|Medical Incident|94115  |4785 |\n",
      "|Medical Incident|94122  |4323 |\n",
      "|Medical Incident|94107  |4284 |\n",
      "|Medical Incident|94133  |3977 |\n",
      "+----------------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTSDF.select(\"CallType\",\"ZipCode\")\n",
    "    .where($\"CallType\".isNotNull)\n",
    "    .groupBy(\"CallType\",\"ZipCode\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-4b) What San Francisco neighborhoods are in the zip codes 94102 and 94103**\n",
    "\n",
    "Let's find out the neighborhoods associated with these two zip codes. In all likelihood, these are some of the contested \n",
    "neighborhood with high reported crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------+\n",
      "|Neighborhood                  |ZipCode|\n",
      "+------------------------------+-------+\n",
      "|Potrero Hill                  |94103  |\n",
      "|Western Addition              |94102  |\n",
      "|Tenderloin                    |94102  |\n",
      "|Nob Hill                      |94102  |\n",
      "|Castro/Upper Market           |94103  |\n",
      "|South of Market               |94102  |\n",
      "|South of Market               |94103  |\n",
      "|Hayes Valley                  |94103  |\n",
      "|Financial District/South Beach|94102  |\n",
      "|Mission Bay                   |94103  |\n",
      "+------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTSDF.select(\"Neighborhood\",\"ZipCode\")\n",
    "    .where($\"ZipCode\" === 94102 || $\"ZipCode\" === 94103)\n",
    "    .distinct()\n",
    "    .show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-5) What was the sum of all calls, average, min and max of the response times for calls?**\n",
    "\n",
    "Let's use the built-in Spark SQL functions to compute the sum, avg, min, and max of few columns:\n",
    "\n",
    "* Number of Total Alarms\n",
    "* What were the min and max the delay in response time before the Fire Dept arrived at the scene of the call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTSDF\n",
    ".select(sum(\"NumAlarms\"), avg(\"ResponseDelayedinMins\"), min(\"ResponseDelayedinMins\"), max(\"ResponseDelayedinMins\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-6a) How many distinct years of data is in the CSV file?**\n",
    "\n",
    "We can use the `year()` SQL Spark function off the Timestamp column data type IncidentDate.\n",
    "\n",
    "In all, we have fire calls from years 2000-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTSDF.select(year($\"IncidentDate\"))\n",
    "    .distinct()\n",
    "    .orderBy(year($\"IncidentDate\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-6b) What week of the year in 2018 had the most fire calls?**\n",
    "\n",
    "**Note**: Week 1 is the New Years' week and week 25 is the July 4 the week. Loads of fireworks, so it makes sense the higher number of calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|weekofyear(IncidentDate)|count|\n",
      "+------------------------+-----+\n",
      "|                      22|  259|\n",
      "|                      40|  255|\n",
      "|                      43|  250|\n",
      "|                      25|  249|\n",
      "|                       1|  246|\n",
      "|                      44|  244|\n",
      "|                      32|  243|\n",
      "|                      13|  243|\n",
      "|                      11|  240|\n",
      "|                       5|  236|\n",
      "|                      18|  236|\n",
      "|                      23|  235|\n",
      "|                       2|  234|\n",
      "|                      42|  234|\n",
      "|                      31|  234|\n",
      "|                      19|  233|\n",
      "|                       8|  232|\n",
      "|                      34|  232|\n",
      "|                      10|  232|\n",
      "|                      21|  231|\n",
      "+------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTSDF.filter(year($\"IncidentDate\") === 2018)\n",
    "    .groupBy(weekofyear($\"IncidentDate\"))\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-7) What neighborhoods in San Francisco had the worst response time in 2018?**\n",
    "\n",
    "It appears that if you living in Presidio Heights, the Fire Dept arrived in less than 3 mins, while Mission Bay took more than 6 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------------------+\n",
      "|Neighborhood                  |ResponseDelayedinMins|\n",
      "+------------------------------+---------------------+\n",
      "|Presidio Heights              |2.8833334            |\n",
      "|Mission Bay                   |6.3333335            |\n",
      "|Chinatown                     |2.65                 |\n",
      "|Financial District/South Beach|3.5333333            |\n",
      "|Tenderloin                    |1.1                  |\n",
      "|Bayview Hunters Point         |4.05                 |\n",
      "|Inner Richmond                |2.5666666            |\n",
      "|Inner Sunset                  |1.4                  |\n",
      "|Sunset/Parkside               |2.6666667            |\n",
      "|South of Market               |1.7666667            |\n",
      "+------------------------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTSDF.select(\"Neighborhood\",\"ResponseDelayedinMins\")\n",
    "    .where(year($\"IncidentDate\") === 2018)\n",
    "    .show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Can not create the managed table('`FireServiceCalls`'). The associated location('file:/C:/Users/mario.serrano/Desktop/FormacionBigData/LearningSpark2_repo/LearningSpark2ndEdition/Ejercicios/spark-warehouse/fireservicecalls') already exists.\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Can not create the managed table('`FireServiceCalls`'). The associated location('file:/C:/Users/mario.serrano/Desktop/FormacionBigData/LearningSpark2_repo/LearningSpark2ndEdition/Ejercicios/spark-warehouse/fireservicecalls') already exists.\r",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:357)\r",
      "  at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:170)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r",
      "  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r",
      "  at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:753)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:731)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:626)\r",
      "  ... 42 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "fireTSDF.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"FireServiceCalls\")\n",
    "\n",
    "val query1 = spark.sql(\"SELECT IncidentNumber, CallType FROM FireServiceCalls LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoT Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceIoTData\r\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Utilización de case class para crear el schema del DataSet\n",
    "case class DeviceIoTData(battery_level: Long, c02_level: Long, cca2: String, cca3: String, \n",
    "                         cn: String, device_id: Long, device_name: String, humidity: Long, \n",
    "                         ip: String, latitude: Double, lcd: String, longitude: Double, scale: String, \n",
    "                         temp: Long, timestamp: Long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- battery_level: long (nullable = true)\n",
      " |-- c02_level: long (nullable = true)\n",
      " |-- cca2: string (nullable = true)\n",
      " |-- cca3: string (nullable = true)\n",
      " |-- cn: string (nullable = true)\n",
      " |-- device_id: long (nullable = true)\n",
      " |-- device_name: string (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lcd: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- scale: string (nullable = true)\n",
      " |-- temp: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|cn           |device_id|device_name          |humidity|ip           |latitude|lcd   |longitude|scale  |temp|timestamp    |\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|8            |868      |US  |USA |United States|1        |meter-gauge-1xbYRYcj |51      |68.161.225.1 |38.0    |green |-97.0    |Celsius|34  |1458444054093|\n",
      "|7            |1473     |NO  |NOR |Norway       |2        |sensor-pad-2n2Pea    |70      |213.161.254.1|62.47   |red   |6.15     |Celsius|11  |1458444054119|\n",
      "|2            |1556     |IT  |ITA |Italy        |3        |device-mac-36TWSKiT  |44      |88.36.5.1    |42.83   |red   |12.83    |Celsius|19  |1458444054120|\n",
      "|6            |1080     |US  |USA |United States|4        |sensor-pad-4mzWkz    |32      |66.39.173.154|44.06   |yellow|-121.32  |Celsius|28  |1458444054121|\n",
      "|4            |931      |PH  |PHL |Philippines  |5        |therm-stick-5gimpUrBB|62      |203.82.41.9  |14.58   |green |120.97   |Celsius|25  |1458444054122|\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creo DataSet transformandolo al schema de la case class DeviceIoTData\n",
    "val ds = spark.read.json(\"../data/iot_devices.json\").as[DeviceIoTData]\n",
    "ds.printSchema\n",
    "ds.show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterTempDS: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterTempDS = ds.filter(d => {d.temp > 30 && d.humidity > 70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceTempByCountry\r\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long, cca3: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsTemp: org.apache.spark.sql.Dataset[DeviceTempByCountry] = [temp: bigint, device_name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsTemp = ds\n",
    "  .filter(d => {d.temp > 25})\n",
    "  .map(d => (d.temp, d.device_name, d.device_id, d.cca3))\n",
    "  .withColumnRenamed(\"_1\", \"temp\")\n",
    "  .withColumnRenamed(\"_2\", \"device_name\")\n",
    "  .withColumnRenamed(\"_3\", \"device_id\")\n",
    "  .withColumnRenamed(\"_4\", \"cca3\").as[DeviceTempByCountry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+---------+--------+----+-------------+\n",
      "|temp|device_name          |device_id|humidity|cca3|cn           |\n",
      "+----+---------------------+---------+--------+----+-------------+\n",
      "|34  |meter-gauge-1xbYRYcj |1        |51      |USA |United States|\n",
      "|28  |sensor-pad-4mzWkz    |4        |32      |USA |United States|\n",
      "|27  |sensor-pad-6al7RTAobR|6        |51      |USA |United States|\n",
      "|27  |sensor-pad-8xUD6pzsQI|8        |35      |JPN |Japan        |\n",
      "|26  |sensor-pad-10BsywSYUF|10       |56      |USA |United States|\n",
      "+----+---------------------+---------+--------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.select($\"temp\", $\"device_name\", $\"device_id\", $\"humidity\", $\"cca3\", $\"cn\")\n",
    "    .where(\"temp > 25\")\n",
    "    .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+---------+---------+----+\n",
      "|temp|device_name          |device_id|device_id|cca3|\n",
      "+----+---------------------+---------+---------+----+\n",
      "|34  |meter-gauge-1xbYRYcj |1        |1        |USA |\n",
      "|28  |sensor-pad-4mzWkz    |4        |4        |USA |\n",
      "|27  |sensor-pad-6al7RTAobR|6        |6        |USA |\n",
      "|27  |sensor-pad-8xUD6pzsQI|8        |8        |JPN |\n",
      "|26  |sensor-pad-10BsywSYUF|10       |10       |USA |\n",
      "+----+---------------------+---------+---------+----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dsTemp2: org.apache.spark.sql.Dataset[DeviceTempByCountry] = [temp: bigint, device_name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsTemp2 = ds\n",
    "    .select($\"temp\", $\"device_name\", $\"device_id\", $\"device_id\", $\"cca3\")\n",
    "    .where(\"temp > 25\")\n",
    "    .as[DeviceTempByCountry]\n",
    "\n",
    "dsTemp2.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 137.0 failed 1 times, most recent failure: Lost task 0.0 in stage 137.0 (TID 3417) (EM2021002716.bosonit.local executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @50b31b82; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @19c27976)\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 137.0 failed 1 times, most recent failure: Lost task 0.0 in stage 137.0 (TID 3417) (EM2021002716.bosonit.local executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @50b31b82; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @19c27976)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)\r",
      "  at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)\r",
      "  ... 42 elided\r",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @50b31b82; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @19c27976)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "val device = dsTemp.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-1) How to detect failing devices with low battery below a threshold?**\n",
    "\n",
    "Note: threshold level less than 8 are potential candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------------------------+\n",
      "|battery_level|c02_level|device_name                |\n",
      "+-------------+---------+---------------------------+\n",
      "|1            |800      |sensor-pad-186516qVnGVN7jK |\n",
      "|1            |800      |sensor-pad-190514UCQcNBzHH |\n",
      "|7            |800      |sensor-pad-1864985T2DlE87SJ|\n",
      "|4            |800      |meter-gauge-1855818YaYf    |\n",
      "|3            |800      |sensor-pad-1886381NkVIS    |\n",
      "+-------------+---------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.select($\"battery_level\",$\"c02_level\",$\"device_name\")\n",
    "    .where($\"battery_level\" < 8)\n",
    "    .sort($\"c02_level\")\n",
    "    .show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-2) How to identify offending countries with high-levels of C02 emissions?**\n",
    "\n",
    "Note: Any C02 levels above 1300 are potential violators of C02 emissions\n",
    "\n",
    "Filter out c02_levels is eater than 1300, sort in descending order on C02_level. Note that this high-level domain specific language API reads like a SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------+--------------+--------------+-------------+-------------+------------------+---------+------------------+\n",
      "|cn                            |avg(battery_level)|avg(c02_level)|avg(device_id)|avg(humidity)|avg(latitude)|avg(longitude)    |avg(temp)|avg(timestamp)    |\n",
      "+------------------------------+------------------+--------------+--------------+-------------+-------------+------------------+---------+------------------+\n",
      "|Solomon Islands               |3.0               |1588.0        |187433.0      |40.0         |-9.43        |159.95            |21.0     |1.458444060894E12 |\n",
      "|Federated States of Micronesia|3.0               |1573.0        |78806.0       |55.0         |6.92         |158.25            |13.0     |1.45844405755E12  |\n",
      "|Rwanda                        |2.5               |1560.5        |102085.0      |44.0         |-2.0         |30.0              |21.5     |1.458444058393E12 |\n",
      "|British Indian Ocean Territory|7.0               |1560.0        |83930.0       |54.0         |-6.0         |71.5              |27.0     |1.458444057649E12 |\n",
      "|Aruba                         |4.0               |1559.0        |99899.0       |84.0         |12.51        |-70.0             |19.5     |1.458444057927E12 |\n",
      "|Isle of Man                   |5.0               |1548.0        |137958.5      |46.5         |54.23        |-4.57             |24.5     |1.4584440593755E12|\n",
      "|Gambia                        |3.0               |1544.5        |34748.0       |66.0         |13.47        |-16.57            |16.0     |1.458444056277E12 |\n",
      "|Lesotho                       |2.5               |1537.5        |74949.0       |72.0         |-29.41       |27.990000000000002|21.0     |1.458444057568E12 |\n",
      "|Cuba                          |5.2               |1534.8        |66819.2       |42.0         |21.444       |-79.336           |26.6     |1.4584440572212E12|\n",
      "|Gabon                         |8.0               |1523.0        |106953.0      |30.0         |0.38         |9.45              |28.0     |1.45844405847E12  |\n",
      "+------------------------------+------------------+--------------+--------------+-------------+-------------+------------------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newDS: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cn: string, avg(battery_level): double ... 7 more fields]\r\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newDS = ds\n",
    "  .where($\"c02_level\" > 1300) // .filter(d=>{d.c02_level > 1300})\n",
    "  .groupBy($\"cn\")\n",
    "  .avg()\n",
    "  .sort($\"avg(c02_level)\".desc)\n",
    "\n",
    "newDS.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-3) Can we sort and group country with average temperature, C02, and humidity?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------+-------------+--------------+\n",
      "|cn                    |avg(temp)|avg(humidity)|avg(c02_level)|\n",
      "+----------------------+---------+-------------+--------------+\n",
      "|Monaco                |34.0     |91.0         |1490.0        |\n",
      "|Anguilla              |34.0     |83.0         |865.0         |\n",
      "|British Virgin Islands|34.0     |81.0         |818.0         |\n",
      "|Turkmenistan          |34.0     |80.0         |900.0         |\n",
      "|Suriname              |34.0     |79.0         |1036.0        |\n",
      "|Gibraltar             |34.0     |78.0         |1127.0        |\n",
      "|Liechtenstein         |34.0     |76.0         |878.0         |\n",
      "|Vanuatu               |33.5     |84.0         |1219.5        |\n",
      "|Cameroon              |33.0     |91.0         |1409.0        |\n",
      "|Fiji                  |33.0     |78.0         |987.0         |\n",
      "+----------------------+---------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.where($\"temp\" > 25 && $\"humidity\" > 75) // filter(d=> {d.temp > 25 && d.humidity > 75})\n",
    "  .select(\"temp\", \"humidity\", \"c02_level\", \"cn\")\n",
    "  .groupBy($\"cn\")\n",
    "  .avg()\n",
    "  .sort($\"avg(temp)\".desc,$\"avg(humidity)\".desc,$\"avg(c02_level)\".desc)\n",
    "  .show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-4) Can we compute min, max values for temperature, C02, and humidity?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------+-------------+--------------+--------------+------------------+------------------+\n",
      "|min(temp)|max(temp)|min(humidity)|max(humidity)|min(c02_level)|max(c02_level)|min(battery_level)|max(battery_level)|\n",
      "+---------+---------+-------------+-------------+--------------+--------------+------------------+------------------+\n",
      "|       10|       34|           25|           99|           800|          1599|                 0|                 9|\n",
      "+---------+---------+-------------+-------------+--------------+--------------+------------------+------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds.select(min(\"temp\"), max(\"temp\"), min(\"humidity\"), max(\"humidity\"), min(\"c02_level\"), max(\"c02_level\"), min(\"battery_level\"), max(\"battery_level\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por \n",
    "defecto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cap2DF: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cap2DF = spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../data/mnm_dataset.csv\")\n",
    "cap2DF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Cuando se define un schema al definir un campo por ejemplo \n",
    "StructField('Delay', FloatType(), True) ¿qué significa el último \n",
    "parámetro Boolean? \n",
    "\n",
    "Si el campo puede ser null o no. En este caso si es nullable.\n",
    "\n",
    "d. Dataset vs DataFrame (Scala). ¿En qué se diferencian a nivel de código? \n",
    "El DataSet nos permite utilizar tanto la API de los DataFrame como la de los RDD, permitiendo utilizar métodos como map, filter, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Utilizando el mismo ejemplo utilizado en el capítulo para guardar en parquet y \n",
    "guardar los datos en los formatos: \n",
    "    1. JSON \n",
    "    2. CSV (dándole otro nombre para evitar sobrescribir el fichero origen) \n",
    "    3. AVRO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:676)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:743)\r",
      "  at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:993)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:311)\r",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r",
      "  ... 42 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "// how to add avro??\n",
    "ds.write.format(\"json\").mode(\"overwrite\").save(\"../data/saved/filejson\")\n",
    "ds.write.format(\"csv\").mode(\"overwrite\").save(\"../data/saved/filecsv\")\n",
    "cap2DF.write.format(\"avro\").mode(\"overwrite\").save(\"../data/saved/fileavro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Revisar al guardar los ficheros (p.e. json, csv, etc) el número de ficheros \n",
    "creados, revisar su contenido para comprender (constatar) como se guardan. \n",
    "    1. A qué se debe que hayan más de un fichero? \n",
    "    2. ¿Cómo obtener el número de particiones de un DataFrame? \n",
    "    3. ¿Qué formas existen para modificar el número de particiones de un DataFrame? \n",
    "    4. Llevar a cabo el ejemplo modificando el número de particiones a 1 y revisar de nuevo el/los ficheros guardados. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res90: Int = 8\r\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newds: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n",
       "res104: Int = 10\r\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newds = ds.repartition(10) // RECOMENDADO PARA INCREMENTAR EL NÚMERO DE PARTICIONES\n",
    "newds.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newds2: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n",
       "res103: Int = 2\r\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newds2 = ds.coalesce(2) // RECOMENDADO PARA REDUCIR EL NÚMERO DE PARTICIONES\n",
    "newds2.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "newds.write.format(\"json\").mode(\"overwrite\").save(\"../data/saved/filejson\")\n",
    "newds.write.format(\"csv\").mode(\"overwrite\").save(\"../data/saved/filecsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
