{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002716.bosonit.local:4041\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1617985782383)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@24c13fe9\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder()\n",
    "    .appName(\"Capítulo 2\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 2\n",
    "\n",
    "    a. Descargar el Quijote \n",
    "    https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79\n",
    "    \n",
    "    Aplicar no solo count (para obtener el número de líneas) y show sino probar \n",
    "    distintas sobrecargas del método show (con/sin truncate, indicando/sin indicar \n",
    "    num de filas, etc) así como también los métodos, head, take, first (diferencias \n",
    "    entre estos 3?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|DON QUIJOTE DE LA...|\n",
      "|Miguel de Cervant...|\n",
      "|                    |\n",
      "|       PRIMERA PARTE|\n",
      "|CAPI?TULO 1: Que ...|\n",
      "|En un lugar de la...|\n",
      "|Tuvo muchas veces...|\n",
      "|En resolucio?n, e...|\n",
      "|historia ma?s cie...|\n",
      "|Deci?a e?l, que e...|\n",
      "|En efecto, remata...|\n",
      "|Imagina?base el p...|\n",
      "|linaje y patria, ...|\n",
      "|Limpias, pues, su...|\n",
      "|Capi?tulo 2: Que ...|\n",
      "|Hechas, pues, est...|\n",
      "|Estos pensamiento...|\n",
      "|Con estos iba ens...|\n",
      "|Autores hay que d...|\n",
      "|muertos de hambre...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|DON QUIJOTE DE LA...|\n",
      "|Miguel de Cervant...|\n",
      "|                    |\n",
      "|       PRIMERA PARTE|\n",
      "|CAPI?TULO 1: Que ...|\n",
      "|En un lugar de la...|\n",
      "|Tuvo muchas veces...|\n",
      "|En resolucio?n, e...|\n",
      "|historia ma?s cie...|\n",
      "|Deci?a e?l, que e...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|value                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|DON QUIJOTE DE LA MANCHA                                                                      |\n",
      "|Miguel de Cervantes Saavedra                                                                  |\n",
      "|                                                                                              |\n",
      "|PRIMERA PARTE                                                                                 |\n",
      "|CAPI?TULO 1: Que trata de la condicio?n y ejercicio del famoso hidalgo D. Quijote de la Mancha|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quijotePath: String = /data/el_quijote.txt\r\n",
       "quijote: org.apache.spark.sql.Dataset[String] = [value: string]\r\n",
       "lineasQuijote: Long = 2186\r\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var quijotePath = \"/data/el_quijote.txt\"\n",
    "var quijote = spark.read.textFile(quijotePath)\n",
    "\n",
    "// Count número de líneas\n",
    "var lineasQuijote = quijote.count()\n",
    "\n",
    "// Utilización método show\n",
    "quijote.show()\n",
    "quijote.show(10)\n",
    "quijote.show(5, false) // sin truncar el resultado, muestra 5 líneas, por defecto es true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: String = DON QUIJOTE DE LA MANCHA\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Métodos head, take y first\n",
    "quijote.head()  // Sin parámetro devuelve la primera fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Array[String] = Array(DON QUIJOTE DE LA MANCHA)\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quijote.take(1) // Necesita parámetro y devuelve siempre un Array, aunque el valor sea 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: String = DON QUIJOTE DE LA MANCHA\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quijote.first() // Devuelve siempre el primer valor, no acepta parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio MnM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "+-----+------+----------+\n",
      "|State| Color|sum(Count)|\n",
      "+-----+------+----------+\n",
      "|   CA|Yellow|    100956|\n",
      "|   WA| Green|     96486|\n",
      "|   CA| Brown|     95762|\n",
      "|   TX| Green|     95753|\n",
      "|   TX|   Red|     95404|\n",
      "|   CO|Yellow|     95038|\n",
      "|   NM|   Red|     94699|\n",
      "|   OR|Orange|     94514|\n",
      "|   WY| Green|     94339|\n",
      "|   NV|Orange|     93929|\n",
      "|   TX|Yellow|     93819|\n",
      "|   CO| Green|     93724|\n",
      "|   CO| Brown|     93692|\n",
      "|   CA| Green|     93505|\n",
      "|   NM| Brown|     93447|\n",
      "|   CO|  Blue|     93412|\n",
      "|   WA|   Red|     93332|\n",
      "|   WA| Brown|     93082|\n",
      "|   WA|Yellow|     92920|\n",
      "|   NM|Yellow|     92747|\n",
      "+-----+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+------+----------+\n",
      "|State| Color|sum(Count)|\n",
      "+-----+------+----------+\n",
      "|   CA|Yellow|    100956|\n",
      "|   CA| Brown|     95762|\n",
      "|   CA| Green|     93505|\n",
      "|   CA|   Red|     91527|\n",
      "|   CA|Orange|     90311|\n",
      "|   CA|  Blue|     89123|\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mnmDF: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\r\n",
       "count_mnmDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, Color: string ... 1 more field]\r\n",
       "caCount_MnmDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cargo el csv en un DataFrame\n",
    "val mnmDF = spark.read.format(\"csv\")\n",
    "  .option(\"header\",\"true\")\n",
    "  .option(\"inferSchema\",\"true\")\n",
    "  .load(\"/data/mnm_dataset.csv\")\n",
    "\n",
    "mnmDF.printSchema // compruebo que se ha creado correctamente\n",
    "\n",
    "// Select del DF los campos \"State\", \"Color\" and \"Count\"\n",
    "// Agrupa el count de Color por state\n",
    "// orderBy() desc\n",
    "\n",
    "val count_mnmDF = mnmDF\n",
    "  .select(\"State\",\"Color\",\"Count\")\n",
    "  .groupBy(\"State\",\"Color\")\n",
    "  .sum(\"Count\")\n",
    "  .orderBy(desc(\"sum(Count)\"))\n",
    "\n",
    "count_mnmDF.show(5, false)\n",
    "\n",
    "// Filtrar por state de California\n",
    "\n",
    "val caCount_MnmDF = mnmDF\n",
    "  .select(\"*\")\n",
    "  .where($\"State\" === \"CA\")\n",
    "  .groupBy(\"State\",\"Color\")\n",
    "  .sum(\"Count\")\n",
    "  .orderBy(desc(\"sum(Count)\"))\n",
    "\n",
    "caCount_MnmDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b. Del ejercicio de M&M aplicar: \n",
    "    \n",
    "    1. Otras operaciones de agregación como el Max con otro tipo de \n",
    "    ordenamiento (descendiente). \n",
    "    2. hacer un ejercicio como el “where” de CA que aparece en el libro pero \n",
    "    indicando más opciones de estados (p.e. NV, TX, CA, CO). \n",
    "    3. Hacer un ejercicio donde se calculen en una misma operación el Max, \n",
    "    Min, Avg, Count. Revisar el API (documentación) donde encontrarán \n",
    "    este ejemplo: \n",
    "    ds.agg(max($\"age\"), avg($\"salary\")) \n",
    "    ds.groupBy().agg(max($\"age\"), avg($\"salary\")) \n",
    "    NOTA: $ es un alias de col() \n",
    "    4. Hacer también ejercicios en SQL creando tmpView "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|State|max(sum(Count))|\n",
      "+-----+---------------+\n",
      "|   CA|         100956|\n",
      "|   WA|          96486|\n",
      "|   TX|          95753|\n",
      "|   CO|          95038|\n",
      "|   NM|          94699|\n",
      "|   OR|          94514|\n",
      "|   WY|          94339|\n",
      "|   NV|          93929|\n",
      "|   AZ|          92287|\n",
      "|   UT|          91341|\n",
      "+-----+---------------+\n",
      "\n",
      "+-----+------+---------------+\n",
      "|State| Color|max(sum(Count))|\n",
      "+-----+------+---------------+\n",
      "|   WY| Green|          94339|\n",
      "|   NV|Orange|          93929|\n",
      "|   WA| Green|          96486|\n",
      "|   CO|Yellow|          95038|\n",
      "|   CA|Yellow|         100956|\n",
      "|   OR|Orange|          94514|\n",
      "|   NM|   Red|          94699|\n",
      "|   UT|Orange|          91341|\n",
      "|   TX| Green|          95753|\n",
      "|   AZ| Brown|          92287|\n",
      "+-----+------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "maxCountDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, max(sum(Count)): bigint]\r\n",
       "leftDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, Color: string ... 1 more field]\r\n",
       "rightDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, max(sum(Count)): bigint]\r\n",
       "joinDF: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 1. Max \n",
    "val maxCountDF = count_mnmDF\n",
    "    .select(\"State\",\"sum(Count)\")\n",
    "    .groupBy(\"State\")\n",
    "    .max(\"sum(Count)\")\n",
    "    .orderBy(desc(\"max(sum(Count))\"))\n",
    "maxCountDF.show()\n",
    "\n",
    "\n",
    "//////////// Prueba Joins\n",
    "val leftDF = count_mnmDF\n",
    "    .select(\"State\",\"Color\",\"sum(Count)\")\n",
    "    .distinct\n",
    "val rightDF = maxCountDF\n",
    "val joinDF = leftDF.as(\"t1\").join(rightDF.as(\"t2\"), \"State\")  // Uso de alias para el where, no haría falta pues las columnas\n",
    "    .where($\"t1.sum(Count)\" === $\"t2.max(sum(Count))\")        // se llama diferentes\n",
    "    .drop(\"sum(Count)\")  // Quito la columna para que no se repitan valores con los máx\n",
    "joinDF.show()  // Muestra una tabla con el color que tiene de Máximo en cada State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|State| Color|sum(Count)|\n",
      "+-----+------+----------+\n",
      "|   CA|Yellow|    100956|\n",
      "|   CA| Brown|     95762|\n",
      "|   NM|   Red|     94699|\n",
      "|   CA| Green|     93505|\n",
      "|   NM| Brown|     93447|\n",
      "|   NM|Yellow|     92747|\n",
      "|   AZ| Brown|     92287|\n",
      "|   AZ| Green|     91882|\n",
      "|   AZ|Orange|     91684|\n",
      "|   CA|   Red|     91527|\n",
      "|   NM|Orange|     91251|\n",
      "|   NM| Green|     91160|\n",
      "|   AZ|Yellow|     90946|\n",
      "|   CA|Orange|     90311|\n",
      "|   NM|  Blue|     90150|\n",
      "|   AZ|   Red|     90042|\n",
      "|   AZ|  Blue|     89971|\n",
      "|   CA|  Blue|     89123|\n",
      "+-----+------+----------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "statesCount_MnmDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// 2. Add más States a la condición\n",
    "val statesCount_MnmDF = mnmDF\n",
    "  .select(\"*\")\n",
    "  .where($\"State\" isin (\"CA\",\"NM\",\"AZ\")) //isin para filtrar de lista, me ahorro código\n",
    "  .groupBy(\"State\",\"Color\")\n",
    "  .sum(\"Count\")\n",
    "  .orderBy(desc(\"sum(Count)\"))\n",
    "\n",
    "statesCount_MnmDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------------+------------+----------+----------+----------+\n",
      "|State| Color|        avg(Count)|count(Count)|min(Count)|max(Count)|sum(Count)|\n",
      "+-----+------+------------------+------------+----------+----------+----------+\n",
      "|   WY| Green|55.657227138643066|        1695|        10|       100|     94339|\n",
      "|   NV|   Red|  55.4944099378882|        1610|        10|       100|     89346|\n",
      "|   UT|  Blue|54.366767371601206|        1655|        10|       100|     89977|\n",
      "|   WA|Orange|55.199638118214715|        1658|        10|       100|     91521|\n",
      "|   NM| Green|  54.1973840665874|        1682|        10|       100|     91160|\n",
      "|   CA|  Blue| 55.59762944479102|        1603|        10|       100|     89123|\n",
      "|   WA|   Red| 55.85397965290245|        1671|        10|       100|     93332|\n",
      "|   NV| Brown| 55.81050090525045|        1657|        10|       100|     92478|\n",
      "|   AZ| Green| 54.82219570405728|        1676|        10|       100|     91882|\n",
      "|   CA|   Red| 55.26992753623188|        1656|        10|       100|     91527|\n",
      "|   AZ|Orange| 54.28300769686205|        1689|        10|       100|     91684|\n",
      "|   CO|  Blue| 55.11032448377581|        1695|        10|       100|     93412|\n",
      "|   NM|Orange|  54.8054054054054|        1665|        10|       100|     91251|\n",
      "|   NM|Yellow| 54.94490521327014|        1688|        10|       100|     92747|\n",
      "|   WY|Orange|55.144827586206894|        1595|        10|       100|     87956|\n",
      "|   UT|Yellow|54.263829787234044|        1645|        10|       100|     89264|\n",
      "|   WY|   Red|54.950898203592814|        1670|        10|       100|     91768|\n",
      "|   OR|  Blue| 54.99756986634265|        1646|        10|       100|     90526|\n",
      "|   NV|Orange|54.865070093457945|        1712|        10|       100|     93929|\n",
      "|   AZ|Yellow| 54.98548972188634|        1654|        10|       100|     90946|\n",
      "+-----+------+------------------+------------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "allMnmDF: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 3. Calcular el máximo, mínimo, count, sum, avg, en la misma consulta\n",
    "val allMnmDF = mnmDF\n",
    "    .select(\"*\")\n",
    "    .groupBy(\"State\",\"Color\")\n",
    "    .agg(avg(\"Count\"),count(\"Count\"),min(\"Count\"),max(\"Count\"),sum(\"Count\"))\n",
    "allMnmDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|State| Color| Total|\n",
      "+-----+------+------+\n",
      "|   CA|Yellow|100956|\n",
      "|   CA| Brown| 95762|\n",
      "|   NM|   Red| 94699|\n",
      "|   CA| Green| 93505|\n",
      "|   NM| Brown| 93447|\n",
      "|   NM|Yellow| 92747|\n",
      "|   AZ| Brown| 92287|\n",
      "|   AZ| Green| 91882|\n",
      "|   AZ|Orange| 91684|\n",
      "|   CA|   Red| 91527|\n",
      "|   NM|Orange| 91251|\n",
      "|   NM| Green| 91160|\n",
      "|   AZ|Yellow| 90946|\n",
      "|   CA|Orange| 90311|\n",
      "|   NM|  Blue| 90150|\n",
      "|   AZ|   Red| 90042|\n",
      "|   AZ|  Blue| 89971|\n",
      "|   CA|  Blue| 89123|\n",
      "+-----+------+------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "statesDF: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 4. Utilizar tempViews\n",
    "mnmDF.createOrReplaceTempView(\"table_states\")\n",
    "var statesDF = spark.sql(\"\"\"SELECT State, Color, SUM(Count) as Total FROM table_states \n",
    "WHERE State in ('CA','AZ','NM') GROUP BY State, Color ORDER BY SUM(Count) DESC\"\"\")\n",
    "statesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
